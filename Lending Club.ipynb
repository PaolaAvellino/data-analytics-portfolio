{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LENDING CLUB - CASE STUDY\n",
    "\n",
    "This project explores the Lending Club dataset, which contains information about loans issued through the platform. The goal is to predict whether a loan will be fully repaid or defaulted by applying the key stages of a Machine Learning workflow: data preparation, model selection, and evaluation.\n",
    "\n",
    "To assess performance, different models are compared and ranked using metrics such as AUC, accuracy, recall, and F1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Description\n",
    "\n",
    "<ul> \n",
    "<li> revol_bal – Total revolving credit balance of the borrower.\n",
    "<li> dti (debt-to-income ratio) - percentage of the borrower’s monthly income allocated to debt payments.\n",
    "<li> funded_amnt_inv – Loan amount funded by investors.\n",
    "<li> revol_util – Revolving credit utilization rate (percentage of available credit in use).\n",
    "<li> annual_inc – Declared annual income of the borrower.\n",
    "<li> funded_amnt – Loan amount approved by the platform.\n",
    "<li> loan_amnt – Loan amount requested by the borrower.\n",
    "<li> term – Loan term (repayment period).\n",
    "<li> grade - Credit grade assigned by the platform based on risk assessment.\n",
    "<li> delinq_2yrs – Number of borrower’s delinquencies in the past 2 years.\n",
    "<li> Fully Paid – Target variable (1 = Fully Paid, 0 = Default). Indicates whether the loan was fully repaid or defaulted.\n",
    "<ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv(\"/Users/paolaavellino/Desktop/GITHUB/lending club loans.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display of the first five rows to get an initial overview of the dataset\n",
    "\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display of the last five rows to review the final columns of the dataset\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check descriptive statistics for the dataset columns\n",
    "\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Confirm the number of rows and columns\n",
    "\n",
    "(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the entire dataset to identify missing values\n",
    "\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "print(df.isnull().sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in each column for easier analysis\n",
    "\n",
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns where all values are missing\n",
    "\n",
    "df = df.dropna(axis=1, how='all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of columns remaining after cleaning\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the percentage of missing values in the columns\n",
    "\n",
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with more than 40% missing values\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"mths_since_last_delinq\", \"mths_since_last_record\", \"next_pymnt_d\",\n",
    "    \"debt_settlement_flag_date\", \"settlement_status\", \"settlement_date\",\n",
    "    \"settlement_amount\", \"settlement_percentage\", \"settlement_term\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that columns with more than 40% missing values were successfully removed\n",
    "\n",
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types and non-null counts of each column\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataset display options to improve performance\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame and its columns\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove four variables considered irrelevant for the model\n",
    "\n",
    "df = df.drop(columns=['desc', 'zip_code', 'emp_title', 'addr_state'])\n",
    "\n",
    "# Remove variables containing post-loan information, as they won't be used in the models\n",
    "\n",
    "df.drop(columns=[\n",
    "    'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',\n",
    "    'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',\n",
    "    'recoveries', 'collection_recovery_fee', 'last_pymnt_d',\n",
    "    'last_pymnt_amnt', 'last_credit_pull_d'\n",
    "], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix columns 'int_rate' and 'revol_util' recognized as objects instead of numeric (float) \n",
    "\n",
    "# Replace commas with dots as decimal separators\n",
    "df['int_rate'] = df['int_rate'].str.replace(',', '.', regex=False)\n",
    "df['revol_util'] = df['revol_util'].str.replace(',', '.', regex=False)\n",
    "\n",
    "# Convert columns to numeric (float)\n",
    "df['int_rate'] = pd.to_numeric(df['int_rate'], errors='coerce')\n",
    "df['revol_util'] = pd.to_numeric(df['revol_util'], errors='coerce')\n",
    "\n",
    "# Verify that the changes were applied correctly\n",
    "print(df[['int_rate', 'revol_util']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns recognized as objects into proper datetime format\n",
    "\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%d/%m/%y', errors='coerce')\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%d/%m/%y', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'emp_length' column values into a numeric format for better analysis\n",
    "\n",
    "def convert_emp_length(emp_length):\n",
    "    if isinstance(emp_length, str):\n",
    "        if \"10+\" in emp_length:\n",
    "            return 10\n",
    "        elif \"< 1 year\" in emp_length:\n",
    "            return 0.5\n",
    "        elif \"year\" in emp_length:\n",
    "            num = ''.join(filter(str.isdigit, emp_length))\n",
    "            return int(num) if num else None\n",
    "        elif emp_length == \"\":\n",
    "            return \"\"\n",
    "        elif \"n/a\" in emp_length.lower():\n",
    "            return None\n",
    "    return None  \n",
    "\n",
    "df['emp_length_num'] = df['emp_length'].apply(convert_emp_length)\n",
    "\n",
    "df[['emp_length', 'emp_length_num']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For the models, I selected variables that do not rely on post-default information and that are relevant for predicting whether a loan will be fully repaid or defaulted.\n",
    "\n",
    "Independent variables:\n",
    "\n",
    "<ul>\n",
    "<li> loan_amnt – Loan amount.\n",
    "<li> int_rate – Interest rate.\n",
    "<li> grade – Loan grade.\n",
    "<li> home_ownership – Type of home ownership.\n",
    "<li> term – Loan term.\n",
    "<li> annual_inc – Annual income.\n",
    "<li> purpose – Loan purpose.\n",
    "<li> emp_length_num – Employment length.\n",
    "</ul>\n",
    "\n",
    "Dependent variable:\n",
    "\n",
    "<li> loan_status – Loan status (target variable).\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical variables into numeric dummy variables for modeling\n",
    "\n",
    "grade = pd.get_dummies(df['grade'],drop_first=True)\n",
    "\n",
    "grade = grade * 1\n",
    "\n",
    "grade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ownership = pd.get_dummies(df['home_ownership'],drop_first=True)\n",
    "\n",
    "home_ownership = home_ownership * 1\n",
    "\n",
    "home_ownership.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = pd.get_dummies(df['term'], drop_first=False)\n",
    "\n",
    "term = term *1\n",
    "\n",
    "term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "purpose = pd.get_dummies(df['purpose'], drop_first=False)\n",
    "\n",
    "purpose = purpose * 1\n",
    "\n",
    "purpose.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'loan_status' with 'Missing' and create dummy variables for modeling\n",
    "\n",
    "df['loan_status'] = df['loan_status'].fillna('Missing')  \n",
    "\n",
    "loan_status = pd.get_dummies(df['loan_status'], drop_first=False)\n",
    "\n",
    "loan_status = loan_status * 1\n",
    "\n",
    "loan_status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the new dummy variable columns to the original DataFrame and drop the original categorical columns no longer needed\n",
    "\n",
    "df = pd.concat([df, grade, home_ownership, term, purpose, loan_status], axis=1)\n",
    "\n",
    "df = df.drop(columns=['grade', 'home_ownership', 'term', 'purpose', 'loan_status'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Evaluation\n",
    "\n",
    "This metric helps assess the discriminative power of the variables.\n",
    "\n",
    "The evaluation criteria are as follows:\n",
    "\n",
    "<ul>\n",
    "<li> IV < 0.02 – Not predictive, provides no relevant information for classification.\n",
    "\n",
    "<li> 0.02 ≤ IV < 0.1 – Weakly predictive.\n",
    "\n",
    "<li> 0.1 ≤ IV < 0.3 – Moderately predictive, contributes to classification.\n",
    "\n",
    "<li> 0.3 ≤ IV < 0.5 – Highly predictive and useful for the model.\n",
    "\n",
    "<li> IV ≥ 0.5 – Extremely predictive.\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the Information Value (IV) of each predictor variable in relation to the target variable (Fully Paid)\n",
    "\n",
    "predictor_vars = [\n",
    "    ' 36 months', ' 60 months', 'B', 'C', 'D', 'E', 'F', 'G', 'NONE', 'OTHER', \n",
    "    'OWN', 'RENT', 'car', 'credit_card', 'debt_consolidation', 'educational',\n",
    "    'home_improvement', 'house', 'major_purchase', 'medical', 'moving', \n",
    "    'other', 'renewable_energy', 'small_business', 'vacation', 'wedding'\n",
    "]\n",
    "\n",
    "\n",
    "def calc_iv(df, feature, target, pr=0):\n",
    "    lst = []\n",
    "    for i in range(df[feature].nunique()):\n",
    "        val = list(df[feature].unique())[i]\n",
    "        lst.append([feature, val, df[df[feature] == val].count()[feature], df[(df[feature] == val) & (df[target] == 1)].count()[feature]])\n",
    "\n",
    "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Bad'])\n",
    "    data = data[data['Bad'] > 0]\n",
    "    data['Share'] = data['All'] / data['All'].sum()\n",
    "    data['Bad Rate'] = data['Bad'] / data['All']\n",
    "    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())\n",
    "    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()\n",
    "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
    "    data['IV'] = (data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])).sum()\n",
    "    data = data.sort_values(by=['Variable', 'Value'], ascending=True)\n",
    "\n",
    "    if pr == 1:\n",
    "        print(data)\n",
    "\n",
    "    return data['IV'].values[0]\n",
    "\n",
    "# Calculate IV for all predictor variables and display results\n",
    "iv_values = {}\n",
    "for var in predictor_vars:\n",
    "    iv_values[var] = calc_iv(df, var, 'Fully Paid')\n",
    "\n",
    "for var, iv in iv_values.items():\n",
    "    print(f\"IV for {var}: {iv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the discriminant analysis, I observed that all the selected variables showed low discriminative power. Therefore, I decided to discard most of them and restart the search for more relevant predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop binary variables with low discriminative power, keeping only 'grade' and 'term'\n",
    "\n",
    "\n",
    "columns_low_iv = [ 'NONE', 'OTHER', 'OWN', 'RENT', 'car', 'credit_card', 'debt_consolidation', \n",
    "                  'educational', 'home_improvement', 'house', 'major_purchase', 'medical', \n",
    "                  'moving', 'other', 'renewable_energy', 'small_business', 'vacation', \n",
    "                  'wedding']\n",
    "\n",
    "\n",
    "df = df.drop(columns=columns_low_iv)\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop additional 'loan_status' categories not needed, keeping only 'Fully Paid' as the target variable\n",
    "\n",
    "df.drop(columns=[\n",
    "    'Charged Off', \n",
    "    'Does not meet the credit policy. Status:Charged Off',\n",
    "    'Does not meet the credit policy. Status:Fully Paid', \n",
    "    'Missing'\n",
    "], inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'title' and 'emp_length' (replaced by numeric 'emp_length_num') to keep the dataset clean. Also drop 'policy_code' as it does not provide relevant information for the model\n",
    "\n",
    "df.drop(columns=['title', 'emp_length'], inplace=True)\n",
    "df.drop(columns=['policy_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all remaining categorical variables into binary dummy variables for evaluation\n",
    "\n",
    "\n",
    "sub_grade = pd.get_dummies(df['sub_grade'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "verification_status = pd.get_dummies(df['verification_status'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "application_type = pd.get_dummies(df['application_type'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "pymnt_plan = pd.get_dummies(df['pymnt_plan'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "initial_list_status = pd.get_dummies(df['initial_list_status'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "hardship_flag = pd.get_dummies(df['hardship_flag'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "disbursement_method = pd.get_dummies(df['disbursement_method'], drop_first=True) * 1\n",
    "\n",
    "\n",
    "debt_settlement_flag = pd.get_dummies(df['debt_settlement_flag'], drop_first=True) * 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dummy variables into the original DataFrame and drop the original categorical versions\n",
    "\n",
    "df = pd.concat([df, sub_grade, verification_status, application_type, pymnt_plan, \n",
    "                initial_list_status, hardship_flag, disbursement_method, debt_settlement_flag], axis=1)\n",
    "\n",
    "df.drop(columns=['sub_grade', 'verification_status', 'application_type', 'pymnt_plan', \n",
    "                 'initial_list_status', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all variables in the dataset (except the target) to identify the most useful predictors for the models\n",
    "\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "def calc_iv(df, feature, target):\n",
    "    lst = []\n",
    "\n",
    "    for val in df[feature].unique():\n",
    "        all_count = df[df[feature] == val].shape[0]\n",
    "        bad_count = df[(df[feature] == val) & (df[target] == 1)].shape[0]\n",
    "\n",
    "        if bad_count == 0 or all_count == bad_count:  \n",
    "            continue\n",
    "\n",
    "        lst.append([feature, val, all_count, bad_count])\n",
    "\n",
    "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Bad'])\n",
    "    \n",
    "    if data.empty:\n",
    "        return 0  \n",
    "\n",
    "    data['Share'] = data['All'] / data['All'].sum()\n",
    "    data['Bad Rate'] = data['Bad'] / data['All']\n",
    "    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())\n",
    "    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()\n",
    "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
    "    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n",
    "\n",
    "    return data['IV'].sum()  \n",
    "\n",
    "iv_results = {col: calc_iv(df, col, 'Fully Paid') for col in df.columns if col != 'Fully Paid'}\n",
    "\n",
    "iv_results_df = pd.DataFrame.from_dict(iv_results, orient='index', columns=['IV'])\n",
    "iv_results_df.sort_values(by='IV', ascending=False, inplace=True)\n",
    "\n",
    "print(iv_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the discriminant analysis results:\n",
    "# - Discard variables with IV < 0.1, as they add little predictive value (exceptions: 'term', 'delinq_2yrs', and 'grade')\n",
    "# - Keep variables with IV between 0.1 and 0.5, as they provide better discriminative power\n",
    "# - Discard variables with IV > 0.5, as they are too aligned with the target and could introduce bias\n",
    "\n",
    "selected_variables = [\n",
    "    'revol_bal', 'dti', 'funded_amnt_inv', \n",
    "    'revol_util', 'annual_inc', \n",
    "    'funded_amnt', 'loan_amnt', ' 36 months', ' 60 months', \n",
    "    'B', 'C', 'D', 'E', 'F', 'G', 'delinq_2yrs'\n",
    "]\n",
    "\n",
    "df = df[selected_variables + ['Fully Paid']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempted to train a Logistic Regression model but encountered an error due to remaining null values\n",
    "\n",
    "print(df.isnull().sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean\n",
    "\n",
    "imputador = SimpleImputer(strategy='mean')\n",
    "\n",
    "\n",
    "df.iloc[:, :-1] = imputador.fit_transform(df.iloc[:, :-1])  \n",
    "\n",
    "print(\"Remaining missing values in the DataFrame:\")\n",
    "print(df.isnull().sum().sum(), \"null values in total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the Logistic Regression process\n",
    "\n",
    "# Split the data into training and test sets\n",
    "\n",
    "X = df.drop(columns=['Fully Paid'])\n",
    "y = df['Fully Paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset: 70% for training and 30% for model evaluation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions: class labels (0 or 1) and probabilities for metrics such as AUC-ROC\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:, 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)  \n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc_logreg = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_logreg:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC-ROC = {roc_auc_logreg:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray') \n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Index for Logistic Regression\n",
    "\n",
    "gini_logreg = 2 *  roc_auc_logreg - 1\n",
    "\n",
    "print(f\"Gini Index (Logistic Regression): {gini_logreg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Results\n",
    "\n",
    "Logistic Regression turned out to be a weak predictor in this case, as its ability to separate the classes is almost negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree model\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\", max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Decision Tree model\n",
    "\n",
    "decision_tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree model\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(decision_tree=decision_tree_model, filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúo el modelo\n",
    "\n",
    "y_pred_tree = decision_tree_model.predict(X_test)\n",
    "y_prob_tree = decision_tree_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_tree)\n",
    "precision = precision_score(y_test, y_pred_tree)\n",
    "recall = recall_score(y_test, y_pred_tree)\n",
    "f1 = f1_score(y_test, y_pred_tree)\n",
    "roc_auc_tree = roc_auc_score(y_test, y_prob_tree)\n",
    "\n",
    "print(\"Decision Tree Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "conf_matrix_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_tree)\n",
    "\n",
    "sns.heatmap(conf_matrix_tree, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_prob_tree)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_tree, tpr_tree, label=f\"AUC-ROC = {roc_auc_tree:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray') \n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Decision Tree\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Index for Decision Tree\n",
    "\n",
    "gini_tree = 2 * roc_auc_tree - 1\n",
    "\n",
    "print(f\"Gini Index (Decision Tree): {gini_tree:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Results\n",
    "\n",
    "Although the Decision Tree improves predictive performance compared to Logistic Regression, it still struggles with correctly classifying the negative class (0). The model tends to favor positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest model\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    max_depth=15,  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5, \n",
    "    class_weight=\"balanced\", \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Random Forest model\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "print(\"Random Forest Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1 Score: {f1_rf:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"AUC-ROC = {roc_auc_rf:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  \n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Random Forest\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Index for Random Forest\n",
    "\n",
    "gini_rf = 2 * roc_auc_rf - 1\n",
    "\n",
    "print(f\"Gini Index (Random Forest): {gini_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Results\n",
    "\n",
    "Random Forest shows better overall performance, reducing the overfitting issues observed in the Decision Tree. Although detecting the negative class remains a challenge, the model proves to be more reliable than the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,  \n",
    "    learning_rate=0.05,  \n",
    "    max_depth=7,  \n",
    "    gamma=0.1,  \n",
    "    subsample=0.8,  \n",
    "    colsample_bytree=0.8,  \n",
    "    scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),  \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the XGBoost model\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n",
    "\n",
    "print(\"XGBoost Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(f\"Precision: {precision_xgb:.4f}\")\n",
    "print(f\"Recall: {recall_xgb:.4f}\")\n",
    "print(f\"F1 Score: {f1_xgb:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_xgb)\n",
    "\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f\"AUC-ROC = {roc_auc_xgb:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray') \n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - XGBoost\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Index for XGBoost\n",
    "\n",
    "gini_xgb = 2 * roc_auc_xgb - 1\n",
    "\n",
    "print(f\"Gini Index (XGBoost): {gini_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"],\n",
    "    \"AUC-ROC\": [0.5142, 0.6598, 0.7275, 0.7285],\n",
    "    \"Gini\": [0.0283, 0.3197, 0.4550, 0.4571],\n",
    "    \"Accuracy\": [0.8045, 0.6949, 0.7574, 0.7155],\n",
    "    \"Precision\": [0.8060, 0.8608, 0.8605, 0.8737],\n",
    "    \"Recall\": [0.9970, 0.7406, 0.8337, 0.7556],\n",
    "    \"F1-score\": [0.8914, 0.7961, 0.8469, 0.8104]\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the results, we can see that the weakest models were Logistic Regression and the Decision Tree. Logistic Regression achieved an AUC-ROC of 0.5142 and a Gini Index of 0.0283, indicating almost no discriminative power, performing only slightly better than random guessing. The Decision Tree, while an improvement over Logistic Regression, still showed limited performance with an AUC-ROC of 0.6598 and a Gini Index of 0.3197. Although this model managed to separate the classes better, it was not effective enough, misclassifying many negative cases as positive and showing relatively low accuracy.\n",
    "\n",
    "In contrast, the Random Forest delivered significantly stronger results, reaching an AUC-ROC of 0.7275 and a Gini Index of 0.4550. This model demonstrated good predictive power and achieved a solid balance between precision and recall, making it a reliable option. Random Forest was particularly effective at capturing positive cases without sacrificing too much precision.\n",
    "\n",
    "However, the best overall performer was XGBoost. It achieved the highest AUC-ROC (0.7285) and Gini Index (0.4571), showing superior ability to separate the classes compared to the other models. Additionally, it maintained high precision and a good balance between sensitivity and specificity. Although its recall was slightly lower than Random Forest’s, XGBoost proved to be the most robust and effective model in this analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redesneuronales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
